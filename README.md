## Attention Variants

We will start with basic Causal self attention mechanism and then see its various optimizations/alternatives.
Idea is to show performance improvements over standard self attention

